from __future__ import annotations

import datetime
import json
import os
import re
import time
from dataclasses import dataclass
from typing import Dict, List, Optional

from playwright.sync_api import sync_playwright

from pipelines.core import run_pipeline
from scraper.fetcher import normalize_url
from scraper.parser import parse_number

AUTH_FILE = "auth_threads.json"


@dataclass
class DiscoveredPost:
    url: str
    snippet: str
    likes: Optional[int] = None
    age_label: Optional[str] = None


def rank_posts(posts: List[DiscoveredPost]) -> List[DiscoveredPost]:
    return sorted(posts, key=lambda p: (p.likes or 0), reverse=True)


def _extract_likes_from_text(text: str) -> Optional[int]:
    if not text:
        return None
    m = re.search(r"([\d\.,]+\s*[KMkm]?)\s*(?:likes?|è®š)", text)
    if not m:
        return None
    return parse_number(m.group(1))


def _extract_age_label(text: str) -> Optional[str]:
    if not text:
        return None
    m = re.search(r"\b\d+\s*[smhdw]\b", text)
    return m.group(0) if m else None


def _clean_snippet(text: str) -> str:
    if not text:
        return ""
    lines = [line.strip() for line in text.splitlines() if line.strip()]
    snippet = " ".join(lines)
    return snippet[:500]


def _harvest_posts(page, seen: Dict[str, DiscoveredPost]):
    anchors = page.query_selector_all('a[href*="/post/"]')
    for anchor in anchors:
        href = anchor.get_attribute("href") or ""
        if "/post/" not in href:
            continue

        if href.startswith("http"):
            url = normalize_url(href.split("?")[0])
        else:
            url = normalize_url(f"https://www.threads.net{href.split('?')[0]}")

        if url in seen:
            continue

        snippet_text = ""
        likes = None
        age_label = None

        try:
            container = anchor.query_selector("xpath=../..") or anchor
            container_text = container.text_content() or ""
            snippet_text = _clean_snippet(container_text)
            likes = _extract_likes_from_text(container_text)
            age_label = _extract_age_label(container_text)
        except Exception:
            snippet_text = ""

        seen[url] = DiscoveredPost(url=url, snippet=snippet_text, likes=likes, age_label=age_label)


def discover_thread_urls(keyword: str, max_posts: int) -> List[DiscoveredPost]:
    if not os.path.exists(AUTH_FILE):
        raise FileNotFoundError("âš ï¸ æ‰¾ä¸åˆ° auth_threads.jsonï¼Œè«‹å…ˆåŸ·è¡Œ scraper/login.pyã€‚")

    headless_flag = os.environ.get("DLENS_HEADLESS", "1") != "0"
    collected: Dict[str, DiscoveredPost] = {}
    target = max_posts

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=headless_flag)
        context = browser.new_context(storage_state=AUTH_FILE)
        page = context.new_page()

        print(f"ğŸ” ä½¿ç”¨é—œéµå­—æœå°‹ï¼š{keyword}")
        page.goto("https://www.threads.net/search", timeout=60000, wait_until="load")
        page.wait_for_load_state("networkidle")
        time.sleep(1.5)

        search_box = None
        try:
            search_box = page.get_by_placeholder("Search").first
            search_box.fill(keyword)
        except Exception:
            try:
                search_box = page.locator('input[type="search"]').first
                search_box.fill(keyword)
            except Exception:
                pass

        if search_box:
            search_box.press("Enter")
        else:
            # fallback: type and press enter on page
            page.keyboard.type(keyword)
            page.keyboard.press("Enter")

        page.wait_for_timeout(1500)
        page.wait_for_load_state("networkidle")
        time.sleep(1)

        print("ğŸ“„ æœå°‹çµæœé å·²è¼‰å…¥ï¼Œé–‹å§‹æ²å‹•...")

        stable_rounds = 0
        last_height = 0
        loop = 0
        while len(collected) < target and stable_rounds < 4:
            loop += 1
            _harvest_posts(page, collected)
            print(f"ğŸ”— ç›®å‰å·²æ”¶é›† {len(collected)} æ¢è²¼æ–‡ URL...")

            page.mouse.wheel(0, 2800)
            page.wait_for_timeout(1200)
            height = page.evaluate("document.body.scrollHeight")
            if height == last_height:
                stable_rounds += 1
            else:
                stable_rounds = 0
            last_height = height

        print(f"âœ… URL ç™¼ç¾å®Œæˆï¼Œæœ€çµ‚å–å¾— {len(collected)} æ¢ï¼ˆé™åˆ¶ï¼š{max_posts}ï¼‰")
        browser.close()

    posts_list = list(collected.values())
    if len(posts_list) > max_posts:
        posts_list = posts_list[:max_posts]
    return posts_list


def ingest_posts(posts: List[DiscoveredPost]):
    total = len(posts)
    for idx, p in enumerate(posts, start=1):
        print("\n==============================")
        print(f"[{idx}/{total}] æ­£åœ¨è™•ç†: {p.url}")
        run_pipeline(p.url, ingest_source="B")
    print(f"\nğŸ‰ äº‹ä»¶çˆ¬èŸ²å®Œæˆï¼Œæœ¬æ¬¡å…±æˆåŠŸè™•ç† {total} æ¢è²¼æ–‡")


def save_hotlist(posts: List[DiscoveredPost], keyword: str) -> str:
    os.makedirs("hotlists", exist_ok=True)
    ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"hotlists/hotlist_{ts}.json"

    data = []
    for p in posts:
        data.append(
            {
                "url": p.url,
                "snippet": p.snippet,
                "likes": p.likes,
                "age_label": p.age_label,
                "keyword": keyword,
                "created_at": datetime.datetime.now().isoformat(),
            }
        )

    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ å·²å„²å­˜ hotlist è‡³ {filename}")
    return filename


def main():
    keyword = input("è«‹è¼¸å…¥é—œéµå­—ï¼ˆä¾‹ï¼šå®ç¦è‹‘ / å¤§ç« / å…¬å±‹ï¼‰ï¼š").strip()
    if not keyword:
        print("âš ï¸ é—œéµå­—ä¸å¯ç‚ºç©º")
        return

    max_posts_raw = input("æœ€å¤šæŠ“å¤šå°‘ç¯‡è²¼æ–‡ï¼Ÿ[é è¨­ 50]ï¼š").strip()
    try:
        max_posts = int(max_posts_raw) if max_posts_raw else 50
    except ValueError:
        max_posts = 50

    mode = input(
        "è¼¸å‡ºæ¨¡å¼ï¼š (1) ç«‹å³ ingest è‡³ Supabase / (2) å…ˆè¼¸å‡º hotlist.json å†æ‰‹å‹• ingest [1/2]ï¼š"
    ).strip()
    if mode not in ("1", "2"):
        mode = "1"

    discovered = discover_thread_urls(keyword, max_posts * 2)
    filtered = discovered
    ranked = rank_posts(filtered)
    final_posts = ranked[:max_posts]

    print(f"âœ… æœ€çµ‚é¸å®š {len(final_posts)} æ¢è²¼æ–‡ï¼ˆmax={max_posts}ï¼‰")

    if mode == "2":
        save_hotlist(final_posts, keyword)
    else:
        ingest_posts(final_posts)


if __name__ == "__main__":
    main()
