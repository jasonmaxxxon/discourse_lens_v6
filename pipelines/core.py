from typing import Callable, Optional

from database.store import save_thread
from scraper.fetcher import fetch_page_html
from scraper.parser import extract_data_from_html


def _log(msg: str, logger: Optional[Callable[[str], None]] = None):
    print(msg)
    if logger:
        logger(msg)


def run_pipeline(
    url: str,
    ingest_source: str | None = None,
    return_data: bool = False,
    logger: Optional[Callable[[str], None]] = None,
):
    _log("\nğŸš€ Pipeline started.", logger)

    # Step 1: fetch HTMLï¼ˆç¾åœ¨æœƒæ‹¿åˆ° initial_html + scrolled_htmlï¼‰
    html_bundle = fetch_page_html(url)
    if not html_bundle or (
        not html_bundle.get("initial_html") and not html_bundle.get("scrolled_html")
    ):
        _log("âŒ ç„¡æ³•æŠ“å– HTML", logger)
        return None

    _log("ğŸ§© HTML OKï¼Œé–‹å§‹è§£æ...", logger)

    # Step 2: parseï¼ˆæœƒå¹«ä½ åˆä½µã€Œåˆå§‹ç•«é¢ Top commentsã€+ã€Œæ·±åº¦æ²å‹•ç•™è¨€ã€ï¼‰
    data = extract_data_from_html(html_bundle, url)
    # attach archive snapshot if present
    data["archive_html"] = html_bundle.get("archive_html")
    data["archive_dom_json"] = html_bundle.get("archive_dom_json")

    # Step 3: result preview
    _log("\n===== çµæœé è¦½ =====", logger)
    _log(f"ä½œè€…: {data['author']}", logger)
    _log(f"ä¸»æ–‡ï¼ˆä¹¾æ·¨ï¼‰: {data['post_text'][:200]} ...", logger)
    _log(f"Like: {data['metrics']['likes']}", logger)
    _log(f"Views: {data['metrics']['views']}", logger)
    _log(f"Reply ç¸½æ•¸ (UI): {data['metrics']['reply_count']}", logger)
    _log(f"Repost ç¸½æ•¸ (UI): {data['metrics']['repost_count']}", logger)
    _log(f"Share ç¸½æ•¸ (UI): {data['metrics']['share_count']}", logger)
    _log(f"å¯¦éš›æŠ“åˆ°ç•™è¨€æ¨£æœ¬: {len(data['comments'])}", logger)
    _log("====================", logger)

    # Step 4: save to DB
    post_id = save_thread(data, ingest_source=ingest_source)
    if post_id:
        data["id"] = post_id
        data["post_id"] = post_id

    # å°ç•™è¨€åˆ—è¡¨
    _log("\n===== ç•™è¨€ Sample =====", logger)
    for idx, c in enumerate(data["comments"], start=1):
        _log(f"\n--- Comment #{idx} ---", logger)
        _log(f"User: {c['user']}", logger)
        _log(f"Likes: {c['likes']}", logger)
        _log(f"Text: {c['text']}", logger)
    _log("======================\n", logger)

    if return_data:
        return data

    return None


def run_pipelines(
    urls: list[str],
    ingest_source: str,
    logger: Optional[Callable[[str], None]] = None,
) -> list[dict]:
    """
    åŸ·è¡Œå¤šå€‹ URL çš„ Pipelineï¼Œå›å‚³è²¼æ–‡ dict listï¼Œä¸¦ä¸€ä½µå¯«å…¥ DBã€‚
    """
    posts: list[dict] = []
    total = len(urls)
    for idx, url in enumerate(urls, start=1):
        _log(f"[{idx}/{total}] è™•ç† {url}", logger)
        data = run_pipeline(url, ingest_source=ingest_source, return_data=True, logger=logger)
        if data:
            posts.append(data)
    return posts
